{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p92rzC-2D71g"
   },
   "source": [
    "# Work with customer dataset of a telecommunications company\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Task\n",
    "\n",
    "Using customer data from a telecommunications company, train a model that predicts their outflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZmr6VeYvLKB"
   },
   "source": [
    "## Initializing a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!killall ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95712,
     "status": "ok",
     "timestamp": 1682669062099,
     "user": {
      "displayName": "Kirill Bogdanov",
      "userId": "01175823895624937540"
     },
     "user_tz": -180
    },
    "id": "UuUc1cxHuymb",
    "outputId": "b14a1c63-36ca-4413-dc2d-59ccbe9799ff"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet\n",
    "!apt install openjdk-8-jdk-headless &> /dev/null\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/jdk-21.jdk/Contents/Home\"\n",
    "\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('/opt/homebrew/bin/ngrok http 4050 &')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8Q743I78vA5_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/04 15:27:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('PySpark_Tutorial').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e55O3aB5vo3k"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1GVOybUCvlXy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparkDataframe = spark.read.option('header', True).option('delimiter', ',').csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sparkDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8amaWNGzDsf"
   },
   "source": [
    "## Understanding the structure of a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------------+-------+----------+------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+--------------+------------+-----+\n",
      "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|Contract|PaperlessBilling|PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+--------------+------------+-----+\n",
      "|  7590-...|Female|            0|    Yes|        No|     1|          No|     No ph...|            DSL|            No|         Yes|              No|         No|         No|             No|Month...|             Yes|     Elect...|         29.85|       29.85|   No|\n",
      "|  5575-...|  Male|            0|     No|        No|    34|         Yes|           No|            DSL|           Yes|          No|             Yes|         No|         No|             No|One year|              No|     Maile...|         56.95|      1889.5|   No|\n",
      "|  3668-...|  Male|            0|     No|        No|     2|         Yes|           No|            DSL|           Yes|         Yes|              No|         No|         No|             No|Month...|             Yes|     Maile...|         53.85|      108.15|  Yes|\n",
      "|  7795-...|  Male|            0|     No|        No|    45|          No|     No ph...|            DSL|           Yes|          No|             Yes|        Yes|         No|             No|One year|              No|     Bank ...|          42.3|     1840.75|   No|\n",
      "|  9237-...|Female|            0|     No|        No|     2|         Yes|           No|       Fiber...|            No|          No|              No|         No|         No|             No|Month...|             Yes|     Elect...|          70.7|      151.65|  Yes|\n",
      "+----------+------+-------------+-------+----------+------+------------+-------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------+----------------+-------------+--------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDataframe.show(5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ivgw172qz9o_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7043"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KmZY25zX0Fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: string (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: string (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: string (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFcKPAI_0cF7"
   },
   "source": [
    "## Conversion of numerical types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customerID',\n",
       " 'gender',\n",
       " 'SeniorCitizen',\n",
       " 'Partner',\n",
       " 'Dependents',\n",
       " 'tenure',\n",
       " 'PhoneService',\n",
       " 'MultipleLines',\n",
       " 'InternetService',\n",
       " 'OnlineSecurity',\n",
       " 'OnlineBackup',\n",
       " 'DeviceProtection',\n",
       " 'TechSupport',\n",
       " 'StreamingTV',\n",
       " 'StreamingMovies',\n",
       " 'Contract',\n",
       " 'PaperlessBilling',\n",
       " 'PaymentMethod',\n",
       " 'MonthlyCharges',\n",
       " 'TotalCharges',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Kihtrqni0-Js"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "sparkDataframe_ = sparkDataframe.select(\n",
    "col('SeniorCitizen').cast('int'), \n",
    "    col('Tenure').cast('Double'), \n",
    "    col('MonthlyCharges').cast('Double'),\n",
    "    col('TotalCharges').cast('Double'), \n",
    "    col('customerID').cast('string'), \n",
    "    col('gender').cast('string'), \n",
    "    col('Partner').cast('string'),\n",
    "    col('Dependents').cast('string'),\n",
    "    col('PhoneService').cast('string'), \n",
    "    col('InternetService').cast('string'), \n",
    "    col('OnlineSecurity').cast('string'),\n",
    "    col('OnlineBackup').cast('string'),\n",
    "    col('DeviceProtection').cast('string'), \n",
    "    col('TechSupport').cast('string'), \n",
    "    col('StreamingTV').cast('string'),\n",
    "    col('StreamingMovies').cast('string'),\n",
    "    col('Contract').cast('string'), \n",
    "    col('PaperlessBilling').cast('string'),\n",
    "    col('PaymentMethod').cast('string'),\n",
    "    col('Churn').cast('string')\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hBBiIm350BD"
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= sparkDataframe_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SeniorCitizen',\n",
       " 'Tenure',\n",
       " 'MonthlyCharges',\n",
       " 'TotalCharges',\n",
       " 'customerID',\n",
       " 'gender',\n",
       " 'Partner',\n",
       " 'Dependents',\n",
       " 'PhoneService',\n",
       " 'InternetService',\n",
       " 'OnlineSecurity',\n",
       " 'OnlineBackup',\n",
       " 'DeviceProtection',\n",
       " 'TechSupport',\n",
       " 'StreamingTV',\n",
       " 'StreamingMovies',\n",
       " 'Contract',\n",
       " 'PaperlessBilling',\n",
       " 'PaymentMethod',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cols)):\n",
    "    sparkDataframe_ = sparkDataframe_.filter(cols[i] +' is not NULL')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMRcAbphNBEP"
   },
   "source": [
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6SZ_rocx79oY"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "sparkDataframe_1 = sparkDataframe_.drop('CustomerID', 'Gender', 'PhoneService', 'MultipleLines', 'Tenure')\n",
    "\n",
    "#Gender does not affect whether a person remains a customer of the company, as well as the telephone service.\n",
    "##Tenure can be obtained as follows Total Charges/Monthly Charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SeniorCitizen',\n",
       " 'MonthlyCharges',\n",
       " 'TotalCharges',\n",
       " 'Partner',\n",
       " 'Dependents',\n",
       " 'InternetService',\n",
       " 'OnlineSecurity',\n",
       " 'OnlineBackup',\n",
       " 'DeviceProtection',\n",
       " 'TechSupport',\n",
       " 'StreamingTV',\n",
       " 'StreamingMovies',\n",
       " 'Contract',\n",
       " 'PaperlessBilling',\n",
       " 'PaymentMethod',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDataframe_2  = sparkDataframe_1.withColumn('Tenure', col('TotalCharges')/col('MonthlyCharges')).drop('TotalCharges',\n",
    "                                                                                                         'MonthlyCharges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX-4GEdHTZWd"
   },
   "source": [
    "## Vectorization of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      " |-- Tenure: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/04 15:27:56 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "sparkDataframe_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0XQ3pjdAcBIL"
   },
   "outputs": [],
   "source": [
    "from pyspark import mllib\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#Converting text columns to numbers via StringIndexer\n",
    "\n",
    "text_cols = [\"SeniorCitizen\", \"Partner\", \"Dependents\", \"InternetService\", \n",
    "                \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \n",
    "                \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\"]\n",
    "\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(sparkDataframe_2) for column in text_cols]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "new_sparkDataframe = pipeline.fit(sparkDataframe_2).transform(sparkDataframe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SeniorCitizen: int, Tenure: double, SeniorCitizen_index: double, Partner_index: double, Dependents_index: double, InternetService_index: double, OnlineSecurity_index: double, OnlineBackup_index: double, DeviceProtection_index: double, TechSupport_index: double, StreamingTV_index: double, StreamingMovies_index: double, Contract_index: double, PaperlessBilling_index: double, PaymentMethod_index: double, Churn_index: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sparkDataframe.drop(\"Partner\", \"Dependents\", \"InternetService\", \n",
    "                \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \n",
    "                \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      " |-- Tenure: double (nullable = true)\n",
      " |-- SeniorCitizen_index: double (nullable = false)\n",
      " |-- Partner_index: double (nullable = false)\n",
      " |-- Dependents_index: double (nullable = false)\n",
      " |-- InternetService_index: double (nullable = false)\n",
      " |-- OnlineSecurity_index: double (nullable = false)\n",
      " |-- OnlineBackup_index: double (nullable = false)\n",
      " |-- DeviceProtection_index: double (nullable = false)\n",
      " |-- TechSupport_index: double (nullable = false)\n",
      " |-- StreamingTV_index: double (nullable = false)\n",
      " |-- StreamingMovies_index: double (nullable = false)\n",
      " |-- Contract_index: double (nullable = false)\n",
      " |-- PaperlessBilling_index: double (nullable = false)\n",
      " |-- PaymentMethod_index: double (nullable = false)\n",
      " |-- Churn_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sparkDataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Gihv_Q00TkOD"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "#After indexing, we use OHE to encode categorical feature\n",
    "\n",
    "features_inp  = [\"Partner_index\", \"Dependents_index\",  \"InternetService_index\", \n",
    "                \"OnlineSecurity_index\", \"OnlineBackup_index\", \"DeviceProtection_index\", \"TechSupport_index\",\n",
    "                 \"StreamingTV_index\", \n",
    "                \"StreamingMovies_index\", \"Contract_index\", \"PaperlessBilling_index\", \"PaymentMethod_index\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [OneHotEncoder(inputCol=column, outputCol=column+\"_ohe\").fit(new_sparkDataframe) for column in features_inp]\n",
    "pipeline = Pipeline(stages=new_columns)\n",
    "new_sparkDataframe_ = pipeline.fit(new_sparkDataframe).transform(new_sparkDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SeniorCitizen: int, Partner: string, Dependents: string, InternetService: string, OnlineSecurity: string, OnlineBackup: string, DeviceProtection: string, TechSupport: string, StreamingTV: string, StreamingMovies: string, Contract: string, PaperlessBilling: string, PaymentMethod: string, Churn: string, Tenure: double, SeniorCitizen_index: double, Partner_index: double, Dependents_index: double, InternetService_index: double, OnlineSecurity_index: double, OnlineBackup_index: double, DeviceProtection_index: double, TechSupport_index: double, StreamingTV_index: double, StreamingMovies_index: double, Contract_index: double, PaperlessBilling_index: double, PaymentMethod_index: double, Churn_index: double, Partner_index_ohe: vector, Dependents_index_ohe: vector, InternetService_index_ohe: vector, OnlineSecurity_index_ohe: vector, OnlineBackup_index_ohe: vector, DeviceProtection_index_ohe: vector, TechSupport_index_ohe: vector, StreamingTV_index_ohe: vector, StreamingMovies_index_ohe: vector, Contract_index_ohe: vector, PaperlessBilling_index_ohe: vector, PaymentMethod_index_ohe: vector]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sparkDataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sparkDataframe_ = new_sparkDataframe_.drop(\"Partner\", \"Dependents\", \"InternetService\", \n",
    "                \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \n",
    "                \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sparkDataframe_ = new_sparkDataframe_.drop(\"Partner_index\", \"Dependents_index\", \"InternetService_index\", \n",
    "                \"OnlineSecurity_index\", \"OnlineBackup_index\", \"DeviceProtection_index\", \"TechSupport_index\", \"StreamingTV_index\", \n",
    "                \"StreamingMovies_index\", \"Contract_index\", \"PaperlessBilling_index\", \"PaymentMethod_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SeniorCitizen: int, Tenure: double, SeniorCitizen_index: double, Churn_index: double, Partner_index_ohe: vector, Dependents_index_ohe: vector, InternetService_index_ohe: vector, OnlineSecurity_index_ohe: vector, OnlineBackup_index_ohe: vector, DeviceProtection_index_ohe: vector, TechSupport_index_ohe: vector, StreamingTV_index_ohe: vector, StreamingMovies_index_ohe: vector, Contract_index_ohe: vector, PaperlessBilling_index_ohe: vector, PaymentMethod_index_ohe: vector]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sparkDataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wOm-Te9bYMia"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "#Combining features into one vector\n",
    "\n",
    "features= list(new_sparkDataframe_.drop('Churn_index').columns)\n",
    "target= 'Churn_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = VectorAssembler(inputCols = features,  outputCol = \"features_vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDataframe_vectorised = vectorizer.transform(new_sparkDataframe_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Tenure: double (nullable = true)\n",
      " |-- SeniorCitizen_index: double (nullable = false)\n",
      " |-- Churn_index: double (nullable = false)\n",
      " |-- Partner_index_ohe: vector (nullable = true)\n",
      " |-- Dependents_index_ohe: vector (nullable = true)\n",
      " |-- InternetService_index_ohe: vector (nullable = true)\n",
      " |-- OnlineSecurity_index_ohe: vector (nullable = true)\n",
      " |-- OnlineBackup_index_ohe: vector (nullable = true)\n",
      " |-- DeviceProtection_index_ohe: vector (nullable = true)\n",
      " |-- TechSupport_index_ohe: vector (nullable = true)\n",
      " |-- StreamingTV_index_ohe: vector (nullable = true)\n",
      " |-- StreamingMovies_index_ohe: vector (nullable = true)\n",
      " |-- Contract_index_ohe: vector (nullable = true)\n",
      " |-- PaperlessBilling_index_ohe: vector (nullable = true)\n",
      " |-- PaymentMethod_index_ohe: vector (nullable = true)\n",
      " |-- features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDataframe_vectorised.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SeniorCitizen',\n",
       " 'Tenure',\n",
       " 'SeniorCitizen_index',\n",
       " 'Churn_index',\n",
       " 'Partner_index_ohe',\n",
       " 'Dependents_index_ohe',\n",
       " 'InternetService_index_ohe',\n",
       " 'OnlineSecurity_index_ohe',\n",
       " 'OnlineBackup_index_ohe',\n",
       " 'DeviceProtection_index_ohe',\n",
       " 'TechSupport_index_ohe',\n",
       " 'StreamingTV_index_ohe',\n",
       " 'StreamingMovies_index_ohe',\n",
       " 'Contract_index_ohe',\n",
       " 'PaperlessBilling_index_ohe',\n",
       " 'PaymentMethod_index_ohe',\n",
       " 'features_vec']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDataframe_vectorised.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDataframe_vectorised = sparkDataframe_vectorised.drop('SeniorCitizen',\n",
    " 'Tenure',\n",
    " 'SeniorCitizen_index',\n",
    " 'Partner_index_ohe',\n",
    " 'Dependents_index_ohe',\n",
    " 'InternetService_index_ohe',\n",
    " 'OnlineSecurity_index_ohe',\n",
    " 'OnlineBackup_index_ohe',\n",
    " 'DeviceProtection_index_ohe',\n",
    " 'TechSupport_index_ohe',\n",
    " 'StreamingTV_index_ohe',\n",
    " 'StreamingMovies_index_ohe',\n",
    " 'Contract_index_ohe',\n",
    " 'PaperlessBilling_index_ohe',\n",
    " 'PaymentMethod_index_ohe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------------------------------------------------------------+\n",
      "|Churn_index|                                                                                 features_vec|\n",
      "+-----------+---------------------------------------------------------------------------------------------+\n",
      "|        0.0|     (25,[1,4,6,7,10,11,13,15,17,19,21,22],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|        0.0|(25,[1,3,4,6,8,9,12,13,15,17,23],[33.17822651448639,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----------+---------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDataframe_vectorised.show(2,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk_1ZJjDqm02"
   },
   "source": [
    "## Creating and training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "siYFALNhq_9F"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(featuresCol='features_vec', labelCol = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "g9PoHvGqvvET"
   },
   "outputs": [],
   "source": [
    "data_train, data_test = sparkDataframe_vectorised.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyGEK9T_wGWd"
   },
   "source": [
    "### Selection of optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "LsRwAy1TwDzF"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(model.regParam, [0.5, 5]).addGrid(model.elasticNetParam, [0.01, 0.1]).addGrid(model.maxIter, [5, 15]).build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "RrGPelUqMeoz"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "eval = BinaryClassificationEvaluator( rawPredictionCol ='prediction', labelCol = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "CtmsG_rxSOOr"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator = model, estimatorParamMaps = grid, evaluator = eval) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "uUxWevjEYOvX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/04 21:53:00 WARN CacheManager: Asked to cache already cached data.\n",
      "24/02/04 21:53:00 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "cv_model = cv.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaIi8In5Zykn"
   },
   "source": [
    "### The best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "kJK-IrWLZbL-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_8d3527345b22', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.01,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='featuresCol', doc='features column name.'): 'features_vec',\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='labelCol', doc='label column name.'): 'Churn_index',\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='maxIter', doc='max number of iterations (>= 0).'): 5,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_8d3527345b22', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = cv_model.bestModel\n",
    "best_model.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "_K1IrbNzd1FX"
   },
   "outputs": [],
   "source": [
    "test_model = cv.fit(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+-----------+----------+\n",
      "|Churn_index|features_vec|rawPrediction|probability|prediction|\n",
      "+-----------+------------+-------------+-----------+----------+\n",
      "|        0.0|    (25,[...|     [-0.2...|   [0.43...|       1.0|\n",
      "|        0.0|    (25,[...|     [-0.2...|   [0.43...|       1.0|\n",
      "|        0.0|    (25,[...|     [-0.2...|   [0.44...|       1.0|\n",
      "|        0.0|    (25,[...|     [0.01...|   [0.50...|       0.0|\n",
      "|        0.0|    (25,[...|     [0.10...|   [0.52...|       0.0|\n",
      "|        0.0|    (25,[...|     [-0.2...|   [0.44...|       1.0|\n",
      "+-----------+------------+-------------+-----------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model.transform(data_test).show(6, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "CBzDaleQhejy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5993365608750224"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.evaluate(test_model.transform(data_test))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
